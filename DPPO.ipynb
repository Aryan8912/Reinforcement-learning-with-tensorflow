{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeSq9lY7rvMcPovL1X5xCA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryan8912/Reinforcement-learning-with-tensorflow/blob/main/DPPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "oqWyL6nU8ww-",
        "outputId": "480e4854-6539-4ad9-ee5b-aea2e0109646"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorflow' has no attribute 'placeholder'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-08790eff17db>\u001b[0m in \u001b[0;36m<cell line: 178>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mGLOBAL_PPO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0mUPDATE_EVENT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mROLLING_EVENT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mUPDATE_EVENT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# not update now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-08790eff17db>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# In TensorFlow 2.x, sessions are not required for most operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# self.sess = tf.Session()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_DIM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'state'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "A simple version of OpenAI's Proximal Policy Optimization (PPO). [https://arxiv.org/abs/1707.06347]\n",
        "\n",
        "Distributing workers in parallel to collect data, then stop worker's roll-out and train PPO on collected data.\n",
        "Restart workers once PPO is updated.\n",
        "\n",
        "The global PPO updating rule is adopted from DeepMind's paper (DPPO):\n",
        "Emergence of Locomotion Behaviours in Rich Environments (Google Deepmind): [https://arxiv.org/abs/1707.02286]\n",
        "\n",
        "View more on my tutorial website: https://morvanzhou.github.io/tutorials\n",
        "\n",
        "Dependencies:\n",
        "tensorflow r1.3\n",
        "gym 0.9.2\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym, threading, queue\n",
        "\n",
        "EP_MAX = 1000\n",
        "EP_LEN = 200\n",
        "N_WORKER = 4                # parallel workers\n",
        "GAMMA = 0.9                 # reward discount factor\n",
        "A_LR = 0.0001               # learning rate for actor\n",
        "C_LR = 0.0002               # learning rate for critic\n",
        "MIN_BATCH_SIZE = 64         # minimum batch size for updating PPO\n",
        "UPDATE_STEP = 10            # loop update operation n-steps\n",
        "EPSILON = 0.2               # for clipping surrogate objective\n",
        "GAME = 'Pendulum-v0'\n",
        "S_DIM, A_DIM = 3, 1         # state and action dimension\n",
        "\n",
        "\n",
        "class PPO(object):\n",
        "    def __init__(self):\n",
        "        # In TensorFlow 2.x, sessions are not required for most operations.\n",
        "        # self.sess = tf.Session()\n",
        "        self.tfs = tf.placeholder(tf.float32, [None, S_DIM], 'state')\n",
        "\n",
        "        # critic\n",
        "        l1 = tf.layers.dense(self.tfs, 100, tf.nn.relu)\n",
        "        self.v = tf.layers.dense(l1, 1)\n",
        "        self.tfdc_r = tf.placeholder(tf.float32, [None, 1], 'discounted_r')\n",
        "        self.advantage = self.tfdc_r - self.v\n",
        "        self.closs = tf.reduce_mean(tf.square(self.advantage))\n",
        "        self.ctrain_op = tf.train.AdamOptimizer(C_LR).minimize(self.closs)\n",
        "\n",
        "        # actor\n",
        "        pi, pi_params = self._build_anet('pi', trainable=True)\n",
        "        oldpi, oldpi_params = self._build_anet('oldpi', trainable=False)\n",
        "        self.sample_op = tf.squeeze(pi.sample(1), axis=0)  # operation of choosing action\n",
        "        self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]\n",
        "\n",
        "        self.tfa = tf.placeholder(tf.float32, [None, A_DIM], 'action')\n",
        "        self.tfadv = tf.placeholder(tf.float32, [None, 1], 'advantage')\n",
        "        # ratio = tf.exp(pi.log_prob(self.tfa) - oldpi.log_prob(self.tfa))\n",
        "        ratio = pi.prob(self.tfa) / (oldpi.prob(self.tfa) + 1e-5)\n",
        "        surr = ratio * self.tfadv                       # surrogate loss\n",
        "\n",
        "        self.aloss = -tf.reduce_mean(tf.minimum(        # clipped surrogate objective\n",
        "            surr,\n",
        "            tf.clip_by_value(ratio, 1. - EPSILON, 1. + EPSILON) * self.tfadv))\n",
        "\n",
        "        self.atrain_op = tf.train.AdamOptimizer(A_LR).minimize(self.aloss)\n",
        "        # In TensorFlow 2.x, you initialize variables differently\n",
        "        # self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def update(self):\n",
        "        global GLOBAL_UPDATE_COUNTER\n",
        "        while not COORD.should_stop():\n",
        "            if GLOBAL_EP < EP_MAX:\n",
        "                UPDATE_EVENT.wait()                     # wait until get batch of data\n",
        "                # self.sess.run(self.update_oldpi_op)     # copy pi to old pi # Session not needed in TF2\n",
        "                # Execute update_oldpi_op immediately\n",
        "                [op.numpy() for op in self.update_oldpi_op]\n",
        "                data = [QUEUE.get() for _ in range(QUEUE.qsize())]      # collect data from all workers\n",
        "                data = np.vstack(data)\n",
        "                s, a, r = data[:, :S_DIM], data[:, S_DIM: S_DIM + A_DIM], data[:, -1:]\n",
        "                # adv = self.sess.run(self.advantage, {self.tfs: s, self.tfdc_r: r}) # Session not needed in TF2\n",
        "                # Calculate advantage directly using TensorFlow operations\n",
        "                adv = (self.tfdc_r - self.v).numpy()\n",
        "                # update actor and critic in a update loop\n",
        "                # [self.sess.run(self.atrain_op, {self.tfs: s, self.tfa: a, self.tfadv: adv}) for _ in range(UPDATE_STEP)] # Session not needed in TF2\n",
        "                # Execute atrain_op directly using TensorFlow operations\n",
        "                # [self.atrain_op.numpy() for _ in range(UPDATE_STEP)] # Incorrect\n",
        "                # update actor and critic in a update loop\n",
        "                for _ in range(UPDATE_STEP):\n",
        "                    # Assuming self.atrain_op is a TensorFlow operation\n",
        "                    self.atrain_op.run()\n",
        "\n",
        "                # [self.sess.run(self.ctrain_op, {self.tfs: s, self.tfdc_r: r}) for _ in range(UPDATE_STEP)] # Session not needed in TF2\n",
        "                # Execute ctrain_op directly using TensorFlow operations\n",
        "                # [self.ctrain_op.numpy() for _ in range(UPDATE_STEP)] # Incorrect\n",
        "                # update actor and critic in a update loop\n",
        "                for _ in range(UPDATE_STEP):\n",
        "                    # Assuming self.ctrain_op is a TensorFlow operation\n",
        "                    self.ctrain_op.run()\n",
        "\n",
        "                UPDATE_EVENT.clear()        # updating finished\n",
        "                GLOBAL_UPDATE_COUNTER = 0   # reset counter\n",
        "                ROLLING_EVENT.set()         # set roll-out available\n",
        "\n",
        "    def _build_anet(self, name, trainable):\n",
        "        with tf.variable_scope(name):\n",
        "            l1 = tf.layers.dense(self.tfs, 200, tf.nn.relu, trainable=trainable)\n",
        "            mu = 2 * tf.layers.dense(l1, A_DIM, tf.nn.tanh, trainable=trainable)\n",
        "            sigma = tf.layers.dense(l1, A_DIM, tf.nn.softplus, trainable=trainable)\n",
        "            norm_dist = tf.distributions.Normal(loc=mu, scale=sigma)\n",
        "        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)\n",
        "        return norm_dist, params\n",
        "\n",
        "    def choose_action(self, s):\n",
        "        s = s[np.newaxis, :]\n",
        "        # a = self.sess.run(self.sample_op, {self.tfs: s})[0] # Session not needed in TF2\n",
        "        # Execute sample_op immediately\n",
        "        a = self.sample_op.numpy()[0]\n",
        "        return np.clip(a, -2, 2)\n",
        "\n",
        "    def get_v(self, s):\n",
        "        if s.ndim < 2: s = s[np.newaxis, :]\n",
        "        # return self.sess.run(self.v, {self.tfs: s})[0, 0] # Session not needed in TF2\n",
        "        # Calculate v directly using TensorFlow operations\n",
        "        return self.v.numpy()[0, 0]\n",
        "\n",
        "\n",
        "class Worker(object):\n",
        "    def __init__(self, wid):\n",
        "        self.wid = wid\n",
        "        self.env = gym.make(GAME).unwrapped\n",
        "        self.ppo = GLOBAL_PPO\n",
        "\n",
        "    def work(self):\n",
        "        global GLOBAL_EP, GLOBAL_RUNNING_R, GLOBAL_UPDATE_COUNTER\n",
        "        while not COORD.should_stop():\n",
        "            s = self.env.reset()\n",
        "            ep_r = 0\n",
        "            buffer_s, buffer_a, buffer_r = [], [], []\n",
        "            for t in range(EP_LEN):\n",
        "                if not ROLLING_EVENT.is_set():                  # while global PPO is updating\n",
        "                    ROLLING_EVENT.wait()                        # wait until PPO is updated\n",
        "                    buffer_s, buffer_a, buffer_r = [], [], []   # clear history buffer, use new policy to collect data\n",
        "                a = self.ppo.choose_action(s)\n",
        "                s_, r, done, _ = self.env.step(a)\n",
        "                buffer_s.append(s)\n",
        "                buffer_a.append(a)\n",
        "                buffer_r.append((r + 8) / 8)                    # normalize reward, find to be useful\n",
        "                s = s_\n",
        "                ep_r += r\n",
        "\n",
        "                GLOBAL_UPDATE_COUNTER += 1                      # count to minimum batch size, no need to wait other workers\n",
        "                if t == EP_LEN - 1 or GLOBAL_UPDATE_COUNTER >= MIN_BATCH_SIZE:\n",
        "                    v_s_ = self.ppo.get_v(s_)\n",
        "                    discounted_r = []                           # compute discounted reward\n",
        "                    for r in buffer_r[::-1]:\n",
        "                        v_s_ = r + GAMMA * v_s_\n",
        "                        discounted_r.append(v_s_)\n",
        "                    discounted_r.reverse()\n",
        "\n",
        "                    bs, ba, br = np.vstack(buffer_s), np.vstack(buffer_a), np.array(discounted_r)[:, np.newaxis]\n",
        "                    buffer_s, buffer_a, buffer_r = [], [], []\n",
        "                    QUEUE.put(np.hstack((bs, ba, br)))          # put data in the queue\n",
        "                    if GLOBAL_UPDATE_COUNTER >= MIN_BATCH_SIZE:\n",
        "                        ROLLING_EVENT.clear()       # stop collecting data\n",
        "                        UPDATE_EVENT.set()          # globalPPO update\n",
        "\n",
        "                    if GLOBAL_EP >= EP_MAX:         # stop training\n",
        "                        COORD.request_stop()\n",
        "                        break\n",
        "\n",
        "            # record reward changes, plot later\n",
        "            if len(GLOBAL_RUNNING_R) == 0: GLOBAL_RUNNING_R.append(ep_r)\n",
        "            else: GLOBAL_RUNNING_R.append(GLOBAL_RUNNING_R[-1]*0.9+ep_r*0.1)\n",
        "            GLOBAL_EP += 1\n",
        "            print('{0:.1f}%'.format(GLOBAL_EP/EP_MAX*100), '|W%i' % self.wid,  '|Ep_r: %.2f' % ep_r,)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    GLOBAL_PPO = PPO()\n",
        "    UPDATE_EVENT, ROLLING_EVENT = threading.Event(), threading.Event()\n",
        "    UPDATE_EVENT.clear()            # not update now\n",
        "    ROLLING_EVENT.set()             # start to roll out\n",
        "    workers = [Worker(wid=i) for i in range(N_WORKER)]\n",
        "\n",
        "    GLOBAL_UPDATE_COUNTER, GLOBAL_EP = 0, 0\n",
        "    GLOBAL_RUNNING_R = []\n",
        "    COORD = tf.train.Coordinator()\n",
        "    QUEUE = queue.Queue()           # workers putting data in this queue\n",
        "    threads = []\n",
        "    for worker in workers:          # worker threads\n",
        "        t = threading.Thread(target=worker.work, args=())\n",
        "        t.start()                   # training\n",
        "        threads.append(t)\n",
        "    # add a PPO updating thread\n",
        "    threads.append(threading.Thread(target=GLOBAL_PPO.update,))\n",
        "    threads[-1].start()\n",
        "    COORD.join(threads)\n",
        "\n",
        "    # plot reward change and test\n",
        "    plt.plot(np.arange(len(GLOBAL_RUNNING_R)), GLOBAL_RUNNING_R)\n",
        "    plt.xlabel('Episode'); plt.ylabel('Moving reward'); plt.ion(); plt.show()\n",
        "    env = gym.make('Pendulum-v0')\n",
        "    while True:\n",
        "        s = env.reset()\n",
        "        for t in range(300):\n",
        "            env.render()\n",
        "            s = env.step(GLOBAL_PPO.choose_action(s))[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lX9dVWB6AUaq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}